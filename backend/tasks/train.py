# tasks/train.py  (step-logging + notify + cooperative stop + continuous status SSE)
import os
import json
import socket
import time
from pathlib import Path

import mlflow
import torch
from celery_app import celery_app
from celery.utils.log import get_task_logger
from mlflow.tracking import MlflowClient

from ml.models.factory import build_model_from_json
from .dataset import create_dataloader_v1, load_training_data
from .tokenizers import choose_tokenizer_from_config

# Redis Pub/Sub (SSE ì¤‘ê³„)
from deps import rds_sync, train_event_channel

# (ì˜µì…˜) í•™ìŠµ ì™„ë£Œ ì‹œ FastAPIë¡œ ì•Œë¦¼ POST
import requests  # noqa: F401

logger = get_task_logger(__name__)

# ===================== DeepSpeed ê´€ë ¨ í•¨ìˆ˜ =====================
def _is_rank0(engine_or_none):
    return (engine_or_none is None) or (getattr(engine_or_none, "global_rank", 0) == 0)

def _maybe_import_deepspeed(use_deepspeed: bool):
    if not use_deepspeed:
        return None
    import os, torch
    os.environ.setdefault("DS_BUILD_OPS", "0")
    os.environ.setdefault("DS_SKIP_CUDA_CHECK", "1")
    if not torch.cuda.is_available():
        raise RuntimeError("GPUê°€ ì—†ëŠ”ë° use_deepspeed=True ì…ë‹ˆë‹¤.")
    import deepspeed
    return deepspeed

# ===================== ìœ í‹¸ =====================
def _as_int(x, default):
    if x is None:
        return int(default)
    try:
        return int(x)
    except Exception:
        return int(default)

def _as_str(x, default):
    return str(x) if x not in (None, "") else str(default)

def _looks_http(uri: str) -> bool:
    return isinstance(uri, str) and uri.startswith(("http://", "https://"))

def _can_connect_http(uri: str, timeout=1.5) -> bool:
    """ê°„ë‹¨ ì†Œì¼“ ì²´í¬ë¡œ MLflow ì„œë²„ ì—°ê²°ì„± í™•ì¸ (íŒŒì¼ ë°±ì—”ë“œëŠ” True)"""
    try:
        from urllib.parse import urlparse
        u = urlparse(uri)
        if u.scheme not in ("http", "https"):
            return True
        if not u.hostname:
            return False
        port = u.port or (443 if u.scheme == "https" else 80)
        with socket.create_connection((u.hostname, port), timeout=timeout):
            return True
    except Exception:
        return False

def _setup_mlflow_tracking():
    """MLFLOW_TRACKING_URI ì ìš© + íƒ€ì„ì•„ì›ƒ ì„¤ì •. HTTP ë¶ˆê°€ì‹œ ê²½ê³ ë§Œ ë‚¨ê¸°ê³  ì§„í–‰."""
    os.environ.setdefault("MLFLOW_HTTP_REQUEST_TIMEOUT", "5")  # ì´ˆ
    uri = os.getenv("MLFLOW_TRACKING_URI", "http://127.0.0.1:5000")
    mlflow.set_tracking_uri(uri)
    if _looks_http(uri) and not _can_connect_http(uri):
        logger.warning("[MLflow] '%s' ì ‘ì† ë¶ˆê°€. run_idë¡œ start_run í•  ë•Œ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", uri)
    logger.info("MLflow Tracking URI = %s", mlflow.get_tracking_uri())

def publish_event(task_id: str, event: str, data: dict):
    """í•™ìŠµ ìƒíƒœë¥¼ Redis Pub/Subìœ¼ë¡œ ì „íŒŒ (ì‹¤íŒ¨í•´ë„ í•™ìŠµì€ ê³„ì†)"""
    try:
        payload = {"event": event, "data": data}
        rds_sync.publish(train_event_channel(task_id), json.dumps(payload, ensure_ascii=False))
    except Exception as e:
        logger.warning(f"[events] publish failed: {e}")

def publish_status(task_id: str, state: str, **extra):
    """ì¼ê´€ëœ status ì´ë²¤íŠ¸ (running/finished/stopped/error)"""
    payload = {"task_id": task_id, "state": state}
    if extra:
        payload.update(extra)
    publish_event(task_id, "status", payload)

# ---------- STOP FLAG helpers (reason ì œê±°) ----------
def _stop_key(task_id: str) -> str:
    return f"train:stop:{task_id}"

def _clear_stop_flag(task_id: str):
    try:
        rds_sync.delete(_stop_key(task_id))
    except Exception:
        pass

def _should_stop(task_id: str) -> bool:
    try:
        flag = rds_sync.get(_stop_key(task_id))
        return flag in ("1", b"1")
    except Exception:
        return False

# ===================== ì†ì‹¤/í‰ê°€ =====================

def calc_loss_batch(input_batch, target_batch, model, device):
    input_batch, target_batch = input_batch.to(device), target_batch.to(device)
    logits = model(input_batch)  # [B, T, V]
    loss = torch.nn.functional.cross_entropy(
        logits.flatten(0, 1),      # [B*T, V]
        target_batch.flatten()     # [B*T]
    )
    return loss

def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = 0.0
    if len(data_loader) == 0:
        return None
    if num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i >= num_batches:
            break
        loss = calc_loss_batch(input_batch, target_batch, model, device)
        total_loss += loss.item()
    return total_loss / max(1, num_batches)

def evaluate_model(model, train_loader, val_loader, device, eval_iter=10):
    model.eval()
    with torch.no_grad():
        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
    model.train()
    return train_loss, val_loss

# ===================== ë©”ì¸ í•™ìŠµ íƒœìŠ¤í¬ =====================

@celery_app.task(bind=True)
def train_and_infer_from_json(self, request_json: dict):
    # ---------- 1) íŒŒë¼ë¯¸í„° íŒŒì‹± ----------
    config         = request_json.get("config", {}) or {}
    layer_json     = request_json.get("model", []) or []
    dataset_name   = request_json.get("dataset", "tiny_shakespeare")
    dataset_config = request_json.get("dataset_config", "default")
    model_name     = request_json.get("modelName", "trained_model")
    run_id         = request_json.get("run_id")  # ë¼ìš°í„°ì—ì„œ ì„ ìƒì„±
    task_id        = self.request.id

    batch_size     = _as_int(config.get("batch_size"), 4)
    epochs         = _as_int(config.get("epochs"), 5)
    seq_max_length = _as_int(config.get("context_length"), 32)
    stride         = _as_int(config.get("stride"), seq_max_length)
    dtype          = _as_str(config.get("dtype"), "fp32")

    # step ë¡œê¹… ì£¼ê¸° / SSE ìŠ¤í… ì´ë²¤íŠ¸ í† ê¸€ / ì™„ë£Œ ì•Œë¦¼ URL
    log_every = _as_int(os.getenv("LOG_EVERY_STEPS"), 1)
    enable_sse_step = os.getenv("ENABLE_SSE_STEP_EVENTS", "0") == "1"
    notify_url = os.getenv("BACKEND_NOTIFY_URL")

    logger.info(
        f"[TASK] start | task_id={task_id}, model_name={model_name}, run_id={run_id}, "
        f"epochs={epochs}, batch_size={batch_size}, dtype={dtype}, "
        f"context_length={seq_max_length}, stride={stride}, log_every={log_every}"
    )
    self.update_state(state="STARTED")

    # run_id í•„ìˆ˜ í™•ì¸
    if not run_id:
        msg = "run_id missing (router must create MLflow run)."
        logger.error(msg)
        publish_event(task_id, "error", {"task_id": task_id, "message": msg})
        publish_status(task_id, "error", message=msg)
        return {"status": "error", "message": msg}

    # ---------- STOP flag ì´ˆê¸°í™” ----------
    _clear_stop_flag(task_id)

    # ---------- 2) MLflow íŠ¸ë˜í‚¹ ì„¤ì • ----------
    _setup_mlflow_tracking()

    try:
        # ---------- 3) í† í¬ë‚˜ì´ì €/ëª¨ë¸ ì¤€ë¹„ ----------
        tokenizer = choose_tokenizer_from_config(config)
        logger.info(f"Tokenizer ready ({config.get('model','gpt-2')}), "
                    f"vocab={getattr(tokenizer, 'n_vocab', 'N/A')}")

        if not isinstance(layer_json, list):
            raise ValueError("layer_json must be a list of layer configurations")

        model = build_model_from_json(layer_json, dtype=dtype)
        total_params     = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        logger.info(f"Model built. total={total_params:,}, trainable={trainable_params:,}")

        # [ì¶”ê°€] LM Head ìœ íš¨ì„± ê²€ì‚¬: GPUë¡œ ë³´ë‚´ê¸° ì „ì— ì°¨ì› ê²€ì‚¬
        vocab_size = getattr(tokenizer, 'n_vocab', None)
        if vocab_size is None and hasattr(tokenizer, 'vocab_size'):
            vocab_size = tokenizer.vocab_size

        if vocab_size and hasattr(model, 'layers') and model.layers:
            last_layer = model.layers[-1]

            if not isinstance(last_layer, torch.nn.Linear) or last_layer.out_features != vocab_size:
                raise ValueError(
                    f"Model's last layer is not a valid LM Head for vocab size {vocab_size}. "
                    f"Expected nn.Linear(out_features={vocab_size}), but found {type(last_layer)} "
                    f"with out_features={getattr(last_layer, 'out_features', 'N/A')}."
                )


        # ---------- 4) ë°ì´í„° ë¡œë“œ/ë¶„í•  ----------
        logger.info(f"Loading dataset: {dataset_name}/{dataset_config}")
        if dataset_name == "allenai/c4" or dataset_name == "roneneldan/TinyStories" or dataset_name == "mychen76/openwebtext-100k":
            training_text = load_training_data(dataset_name, dataset_config,
                                   split="train", 
                                   streaming=True, max_rows=50000)
        else:
            training_text = load_training_data(dataset_name, dataset_config)
        training_text = training_text[:]  # ìƒ˜í”Œ ì»· (ì›í•˜ë©´ ì œê±°)
        split_idx = int(len(training_text) * 0.8)
        train_text, val_text = training_text[:split_idx], training_text[split_idx:]

        # ---------- 5) ë°ì´í„°ë¡œë” ----------
        train_loader = create_dataloader_v1(
            txt=train_text, batch_size=batch_size,
            max_length=seq_max_length, stride=stride,
            shuffle=True, num_workers=0, tokenizer=tokenizer,
        )
        val_loader = create_dataloader_v1(
            txt=val_text, batch_size=batch_size,
            max_length=seq_max_length, stride=stride,
            shuffle=False, num_workers=0, tokenizer=tokenizer,
        )
        logger.info(f"Dataloaders ready. train_batches={len(train_loader)}, val_batches={len(val_loader)}")

        # ---------- 6) ì¥ë¹„/ì˜µí‹°ë§ˆì´ì € ----------
        use_deepspeed = False 
        deepspeed = _maybe_import_deepspeed(use_deepspeed)
        ds_engine = None

        if use_deepspeed:
            assert deepspeed is not None, "DeepSpeed not installed"
            
            # ë°±ì—”ë“œ ê¸°ë³¸ê°’(ì—†ìœ¼ë©´) + í”„ë¡ íŠ¸ ì˜¤ë²„ë¼ì´ë“œ
            ds_cfg = json.load(open("tasks/files/deepspeed_config.json"))
            # DeepSpeedê°€ optimizerë¥¼ ë§Œë“¤ì§€ ì•Šìœ¼ë©´, ê¸°ì¡´ optimizerë¥¼ ë„˜ê¸¸ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
            # ì—¬ê¸°ì„  DSê°€ ì§ì ‘ ìƒì„±(ê¶Œì¥): model_parametersë§Œ ë„˜ê¹€
            ds_engine, optimizer, _, _ = deepspeed.initialize(
                model=model,
                model_parameters=(p for p in model.parameters() if p.requires_grad),
                config=ds_cfg,
            )
            model = ds_engine           # ì´í›„ í•™ìŠµì—ì„œ modelë¡œ ì‚¬ìš©
            device = ds_engine.device
        else:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            model = model.to(device)
            optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)

        eval_freq, eval_iter = 1, 5
        global_step = 0
        last_train_loss = None
        last_val_loss = None

        logger.info(f"Training on device: {device}")

        # ---------- 7) MLflow runì— ë¶™ê¸° + started ì´ë²¤íŠ¸ ----------
        mlflow_url = None
        exp_id = None
        try:
            client = MlflowClient()
            run_info = client.get_run(run_id).info
            exp_id = run_info.experiment_id
            tracking_uri = mlflow.get_tracking_uri().rstrip("/")
            mlflow_url = f"{tracking_uri}/#/experiments/{exp_id}/runs/{run_id}"
            logger.warning(f"ğŸƒ View run at: {mlflow_url}")
            logger.warning(f"ğŸ§ª View experiment at: {tracking_uri}/#/experiments/{exp_id}")
        except Exception as e:
            logger.warning("[MLflow] get_run ì‹¤íŒ¨: %s", e)

        # ì‹œì‘ ì•Œë¦¼
        publish_event(task_id, "started", {
            "task_id": task_id,
            "model_name": model_name,
            "run_id": run_id,
            "mlflow_url": mlflow_url,
            "mlflow_experiment_id": exp_id,
            "epochs": epochs,
            "batch_size": batch_size,
            "context_length": seq_max_length,
            "stride": stride
        })
        publish_status(task_id, "running", epoch=0, epochs=epochs, global_step=0)

        # ---------- 8) í•™ìŠµ + (ìŠ¤í…/ì—í¬í¬) ë¡œê¹… ----------
        try:
            # ì¤‘ì²© run ë°©ì§€
            if mlflow.active_run():
                mlflow.end_run()

            ema = None
            ema_alpha = float(os.getenv("STEP_LOSS_EMA_ALPHA", "0.1"))

            with mlflow.start_run(run_id=run_id):
                for epoch in range(epochs):
                    model.train()
                    epoch_loss, batch_count = 0.0, 0

                    for step_idx, (input_batch, target_batch) in enumerate(train_loader, start=1):
                        # í˜‘ë ¥ì  ì¤‘ë‹¨: ë§¤ ìŠ¤í… ì§ì „ì— í”Œë˜ê·¸ í™•ì¸
                        if _should_stop(task_id):
                            # í•™ìŠµ ì¤‘ë‹¨ ì²˜ë¦¬
                            try:
                                mlflow.set_tag("training_status", "stopped_by_user")
                                mlflow.log_metric("stopped_at_global_step", global_step)
                                mlflow.log_metric("stopped_at_epoch", epoch + 1)
                            except Exception:
                                pass

                            publish_event(task_id, "stopped", {
                                "task_id": task_id,
                                "global_step": global_step,
                                "epoch": epoch + 1,
                                "status": "stopped"
                            })
                            publish_status(task_id, "stopped", epoch=epoch+1, global_step=global_step)

                            # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
                            try:
                                del train_loader, val_loader, optimizer
                                if torch.cuda.is_available():
                                    torch.cuda.empty_cache()
                            except Exception:
                                pass

                            return {
                                "status": "stopped",
                                "message": "Training stopped by request.",
                                "stopped_at_step": global_step,
                                "stopped_at_epoch": epoch + 1
                            }

                        if ds_engine is not None:
                            model.zero_grad()
                            loss = calc_loss_batch(input_batch, target_batch, model, device)
                            model.backward(loss)   # ds_engine.backward
                            model.step()           # ds_engine.step (grad accumulationì€ ds_configê°€ ë°˜ì˜)
                        else:
                            optimizer.zero_grad()
                            loss = calc_loss_batch(input_batch, target_batch, model, device)
                            loss.backward()
                            optimizer.step()

                        # ---------- ìŠ¤í… ë‹¨ìœ„ ----------
                        global_step += 1
                        epoch_loss  += loss.item()
                        batch_count += 1

                        # EMA ê°±ì‹ 
                        ema = loss.item() if ema is None else (ema_alpha * loss.item() + (1 - ema_alpha) * ema)

                        # ì£¼ê¸°ì  ë¡œê¹…
                        if (global_step % log_every) == 0:
                            mlflow.log_metric("train/step_loss", loss.item(), step=global_step)
                            mlflow.log_metric("train/step_loss_ema", ema, step=global_step)
                            logger.info(f"[step {global_step}] loss={loss.item():.4f} | ema={ema:.4f}")

                            if enable_sse_step:
                                publish_event(task_id, "step", {
                                    "task_id": task_id,
                                    "global_step": global_step,
                                    "epoch": epoch + 1,
                                    "loss": float(loss.item()),
                                    "ema_loss": float(ema),
                                })
                                # ìƒíƒœë„ ê°™ì´ í•œ ë²ˆ ë”
                                publish_status(task_id, "running", epoch=epoch+1, global_step=global_step)

                    # ---------- ì—í¬í¬ ë‹¨ìœ„ ----------
                    avg_epoch = epoch_loss / max(batch_count, 1)
                    mlflow.log_metric("train/epoch_loss", avg_epoch, step=epoch)
                    logger.info(f"Epoch {epoch+1}/{epochs} | avg_loss={avg_epoch:.4f}")

                    # ì •ê¸° í‰ê°€ + ì§„í–‰ ì´ë²¤íŠ¸
                    last_train_loss, last_val_loss = evaluate_model(
                        model, train_loader, val_loader, device, eval_iter=eval_iter
                    )
                    if last_train_loss is not None:
                        mlflow.log_metric("eval/train_loss", last_train_loss, step=epoch)
                    if last_val_loss is not None:
                        mlflow.log_metric("eval/val_loss", last_val_loss, step=epoch)

                    publish_event(task_id, "progress", {
                        "task_id": task_id,
                        "epoch": epoch + 1,
                        "epochs": epochs,
                        "avg_epoch_loss": avg_epoch,
                        "train_loss": last_train_loss,
                        "val_loss": last_val_loss,
                        "global_step": global_step,
                        "run_id": run_id
                    })
                    # ì§„í–‰ ìƒíƒœ ê°±ì‹  (ì—í¬í¬ë§ˆë‹¤ ìµœì†Œ 1íšŒ)
                    publish_status(task_id, "running", epoch=epoch+1, epochs=epochs, global_step=global_step)

                # ---------- 9) í•™ìŠµ ê²°ê³¼ ì €ì¥ ----------
                COMPLETED_DIR = Path("completed")
                COMPLETED_DIR.mkdir(exist_ok=True)
                save_path = COMPLETED_DIR / f"{model_name}.pt"

                bundle = {
                    "layers": layer_json,
                    "config": {
                        **config,
                        "dtype": dtype,
                        "context_length": seq_max_length,
                    },
                    "state_dict": model.state_dict(),
                }
                
                if ds_engine is not None:
                    if _is_rank0(ds_engine):
                        bundle["state_dict"] = ds_engine.module.state_dict()
                        torch.save(bundle, save_path)
                    if ds_engine is not None:
                        ds_engine.barrier()  # ëª¨ë“  ë­í¬ ë™ê¸°í™”
                else:
                    torch.save(bundle, save_path)

                # ì•„í‹°íŒ©íŠ¸ ê¸°ë¡ (ì‹¤íŒ¨í•´ë„ í•™ìŠµì€ ì™„ë£Œë¡œ ê°„ì£¼)
                try:
                    mlflow.log_artifact(str(save_path))
                except Exception as e:
                    logger.warning("[MLflow] artifact log skipped: %s", e)

        except Exception:
            # í•„ìš”ì‹œ no-mlflow ê²½ë¡œë¡œ ì „í™˜ ê°€ëŠ¥. ì§€ê¸ˆì€ ì—ëŸ¬ ì „íŒŒ.
            raise

        # ---------- 10) ì™„ë£Œ ì•Œë¦¼ ----------
        finished_payload = {
            "task_id": task_id,
            "model_name": model_name,
            "run_id": run_id,
            "completed_model_path": str(save_path),
            "last_train_loss": last_train_loss,
            "last_val_loss": last_val_loss,
            "status": "finished",
            "ts": time.time(),
        }

        publish_event(task_id, "finished", finished_payload)
        publish_status(task_id, "finished")

        # (ì˜µì…˜) FastAPI ë¼ìš°íŠ¸ë¡œ POST
        if notify_url:
            try:
                resp = requests.post(notify_url, json=finished_payload, timeout=5)
                resp.raise_for_status()
                logger.info(f"[Notify] POST ì„±ê³µ â†’ {notify_url}")
            except Exception as e:
                logger.warning(f"[Notify] POST ì‹¤íŒ¨: {e}")

        return {
            "status": "success",
            "message": "Training complete",
            "completed_model_path": str(save_path),
        }

    except Exception as e:
        logger.exception("Training failed")
        err_payload = {
            "task_id": task_id,
            "model_name": model_name if 'model_name' in locals() else None,
            "run_id": run_id if 'run_id' in locals() else None,
            "message": str(e)
        }
        publish_event(task_id, "error", err_payload)
        publish_status(task_id, "error", message=str(e))
        raise
